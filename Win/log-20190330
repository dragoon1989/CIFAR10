1、使用relu作为激活函数，没有dropout，只训练20个epoch
所有b初始化为0，w使用he normal分布初始化。
（1）使用Adam作为优化器，初始lr=1e-3：
（2）使用GradientDescend作为优化器，初始lr=1e-5：训练非常缓慢，精度缓慢上升到约27%
（3）使用GradientDescend作为优化器，初始lr=1e-4：训练缓慢，精度缓慢上升到约43%
（3）使用GradientDescend作为优化器，初始lr=1e-3：精度逐渐上升到约63%
这说明使用he normal作为初始化，确实减少了relu激活造成的death，模型训练对lr的容忍度大幅度提高了。但是使用固定的lr和GradientDescend训练确实速度比较慢，这一点仍然不如Adam这种自适应lr的优化器。